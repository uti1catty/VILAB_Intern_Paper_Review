# [ECCV22] Adaptive Spatial-BCE Loss for Weakly Supervised Semantic Segmentation  
github: https://github.com/allenwu97/Spatial-BCE

# 1. Introduction
기존의 BCE loss는 whole probability map의 average를 통해 계산하여 fg/bg상관없이 모든 pixel을 same direction으로 계산.  
-> reduce discrimination btw fg and bg.  

Spatial-BCE loss는 fg와 bg pixels를 different direction으로 optimize.  
Spatial BCE Loss는 각 pixel마다 independently loss calcuate하도록 디자인됨.  
optimization direction은 probability in the prediction map과 threshold로 결정.  

Imag level label은 fg / bg의 pixel level 구분 guidance가 없음.  
Thus we introduce auxiliary task to constrain(강요) the distribution(분배) of pixel probability, making Spatial-BCE Loss correctly assign the optimization direction for each pixel.  
이 auxiliary task는 self-supervised로 extra data가 필요 없음.  

fg와 bg를 나누는 threshold를 기존 methods는 fixed threshold를 사용했으나 여기는 network가 학습과정에서 threshold를 optimize하도록 함.  
>A better way is to generate it through the network attentively for diverse inputs. Therefore, we design
an alternate training strategy to alternately optimize this threshold.  

# 3. Approach
## 3.1 Overview
모든 pixel을 average하여 prediction을 만드는 경우 discimination of features between different pixels가 ignore됨. (non-target pixels, non-discriminative target pixel들 )  
large threshold의 경우 pseudo label이 patial object에만 activate.  
small threshold는 non-target regions surrounding the boundary가 함께 activate.  

기존의 BCE loss는 image I에 대해 feature map F를 만든 후 
P'^c = Sigmoid(Linear^c(GAP(F))) 를 통해 class c에 대한 prediction 생성.  
(F를 GAP한 후 Linear layer를 통과하여 Sigmoid 수행)  

Spatial BCE에서는 probability map P^c = Sigmoid(Linear^c(F)) 를 생성.  
P^c = {pi^c} (w x h)  
Spatial-BCE Loss = 모든 class, 모든 pixel(hw)에 대하여 y^c x R(pi^c) - (1 - y^c)log(1 - pi^c) 합. (eq 1)  
CE를 수행하는데 여기서 해당 class가 있는 경우 R()을 pixel i의 probability pi^c에 수행.  
R()은 pixel-wise re-factoring strategy for positive categories.  

## 3.2 Spatial-BCE Loss
category specific threshold t^c가 pre-defined되었다 가정하자.  
target candidate: pixel (pi^c > t^c)  
non-target candidate: pixel (pi^c <= t^c)  
Spatial-BCE Loss는 taret candidate와 non target candidate를 분리하면서 uncertain canddiate를 R()로 Penalize한다.  
즉 둘 사이의 확실한 분리를 목적으로 함.  

R() design을 위한 principle
1. target candidate의 'uncertainty'는 probability가 증가함에 따라 단조 감소(monotonically decrease)한다. probability가 1이면 0이 된다.  
2. non-target candidate의 'uncertainty'는 probability가 감소함에 따라 단조 감소한다. probability가 0이면 0이 된다.  
3. 'uncertainty'는 threshold 근처에서 최대가 된다.  

tc와 ntc를 위한 R()을 따로 정의  
R_tc(pi^c) = - alpha x (pi^c - t^c)^2 + 1 (alpha = (1 - t^c)^(-2))   
pi^c는 (t^c, 1)내에 존재.  
R_ntc(pi^c) = - beta x (pi^c - t^c)^2 + 1 (beta = (t^c)^(-2))  
pi^c는 (0, t^c)에 존재.  
R^tc는 위의 1, 3을 지켜 p가 1에서 0이 되고 t^c에서 1이 된다.  
R_ntc는 위의 2, 3을 지켜 p가 0에서 0이 되고 t^c에서 1이 된다.  

즉 앞의 Loss 식에서 (1 - y^c), 없는 class에 대해서는 모든 pixel의 p가 작을 때 loss가 최소가 되고,  
y^c, 있는 class에 대해서는 각 pixel의 uncertatinty가 작아지도록, 즉 target t^c에서 멀어져 더 확실히 fg/bg로 나누어지도록 강요받음.  
=> Q) 그렇다면 non-target candidate의 p값은 어느 방향으로 이동하는가? 값이 더 작아지는 방향으로 이동하는가?   
=> A) Figure 3에서 Gradient를 따라 이동하게 된다면 그렇게 이동할 것.  
=> Q) 그렇다면 해당 class이지만 p값이 작게 유도되어 non-target candidate로 분류된 pixel은 어떻게 다시 올릴 것인가?  

## 3.3 Auxiliary Supervision
Spatial-BCE Loss 식을 보면 모든 pixel의 p값이 0이되면 모든 uncertanty가 0이 되고 non class값의 loss도 모두 0이되어 Loss가 0이 된다.  
이렇게 수렴하는 것을 막기 위해 Target, Non-Target proportion (T/NT)을 유지하도록 만든다.  

먼저 image-level label로 classification network by BCE Loss를 학습하여 rough pseudo-labels를 생성.  
dCRF를 수행한 후 Q^c를 계산.  
Q^c = c로 label된 pixel 개수 / c가 아닌 class로 label된 pixel 개수.  
즉 Q^c는 Class c에 대하여 fg/bg 분포.  
Q^c를 각 image마다 계산한다.  

Spatial-BCE Loss로 계산한 결과에서 P^c^hat을 계산.  
P^c^hat = (모든 pixel에 대하여 pi^c 합) / (모든 pixel에 대하여 (1 - pi^c) 합).  
P^c^hat의 분자는 fg가 많으면 값이 오르고 분모는 bg가 많으면 값이 오름.  
즉 P^c^hat은 fg/bg 비율  
Kullback-Leibler (KL) Divergence 를 통해 Q^c와 P^c^hat 사이의 괴리를 계산, 이 값을 loss로 사용하여 둘 사이의 차이를 줄이도록 강요.  

D_KL = image에 존재하는 class들에 대하여 yc x P^c^hat x log(P^c^hat / Q^c) 의 합  
Q^c는 매 8k iteration마다 새로 계산해주었다.  
initial Q^c가 정확한 값을 제공하지는 않지만 distribution of pixel의 guide로써 충분한 역할을 해준다.  

L = L_Spatial-BCE + D_KL  

## 3.4 Adaptive Threshold  
r iteration을 'training phase' 단위로 나눔.  
앞의 theta iteration동안 threshold t를 T/NT proportion Q^c로 initialize.  
Q^c / (1 + Q^c) 로 positive category c의 target pixel 비율을 계산하고
Q^c / (1 + Q^c) -th percentile of predicted probabilities P^c를 image-specific category threshold t^c로 사용.  
=> Q) P^c의 p값을 순서대로 나열했을 때 (Q^c / (1+Q^c)) % 위치의 p값을 t로 본다는 뜻인가?  

이때 network는 only gradient of loss w.r.t P에 의해서만 update.  

rest r-theta iteration동안 t^c를 network through convolution layers를 통해 생성하고 update it via the Spatial-BCE loss.   
=> Q) 무슨 뜻인가? t를 어떻게 생성한다는 뜻?  
이때 gradient of loss w.r.t P is detached. (t와 P를 동시에 학습시키는 것은 network training에 불안정을 가져오기 때문.)  

### 3.4.1 Pseudo label Generation
inference동안 directly use t generated by network to as the image-specific threshold of fg and bg.  
li = argmax(Si^c) if max(Si^c) > 0, bg otherwise.  
Si^c = {pi^c - t^c, c는 Pos 내의 class}  

# 4 Experiments
## 4.1 Datasets and Evaluation Metric
PASCAL VOC 2012 - augmented 10582 train dataset 포함.  
MS-COCO 2014 - fg 없는 iamge 제거. gt에 겹치는 pixel을 위해 use annotation of COCO-Stuff as an alternative, which share the same image set with MS-COCO.  
## 4.2 Implementation detail  
ResNet38 as backbone of classifcation network.  
random crop 368 x 368, RandomAug during Training.  
pseudo label 생성 이후 IRN, saliency maps generated by PoolNet 사용 as post processing.  
Sementatic Segmentation Network: Deeplab-LargeFOV, Deeplab-ASPP (backbone: ResNet101, VG16)  
모든 backbone은 ImageNet pretrained.  

